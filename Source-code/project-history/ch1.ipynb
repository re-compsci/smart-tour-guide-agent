{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Smart Tour Guide Bot</h1>\n",
    "\n",
    "**Project Title:**\n",
    "Multimodal AI Travel Assistant\n",
    "\n",
    "**Names:**\n",
    "\n",
    "- Renad Naser\n",
    "- â Amjad Althagafi\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "In a world with many transportations and countries, many people are becoming excited about the visiting new places and discover new things. \n",
    "Nowadays, the internet is full with many resources which make it difficult to find a specific information with being sure that it is correct or enough.\n",
    "\n",
    "So we design and deploy an intelligent AI chatbot that answers user travel-related questions using voice or text, by extracting and summarizing data from global travel guide sources.\n",
    "\n",
    "**Description:**\n",
    "\n",
    "This project aims to build a multimodal RAG chatbot that allows users to ask questions and requests, like â€”â€œWhat can I visit in Paris in 2 days?â€â€”and receive instant, accurate responses. The chatbot uses speech recognition, and summarize relevant information from tourist guides like Wikivoyage. It supports both text and voice input and provides responses using pre-trained transformer models.\n",
    "\n",
    "**AI Techniques:**\n",
    "\n",
    "\tâ€¢\tSpeech Recognition (Whisper by OpenAI)\n",
    "\tâ€¢\tText Chunking & Embedding (LangChain + HuggingFace Transformers)\n",
    "\tâ€¢\tVector Search (ChromaDB or FAISS DB)\n",
    "\tâ€¢\tMultimodal RAG system\n",
    "\tâ€¢\tConversational Agents with Memory (LangChain Agents)\n",
    "\tâ€¢\tLangSmith for evaluation\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "\tâ€¢\tSmart travel planning assistants\n",
    "\tâ€¢\tAccessibility tools for visually/hearing-impaired travelers\n",
    "\tâ€¢\tEducational tools for learning about geography and culture\n",
    "\tâ€¢\tChatbots for tourism websites or mobile travel apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: speechrecognition in c:\\users\\riina\\anaconda3\\lib\\site-packages (3.14.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\riina\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\riina\\anaconda3\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: PyAudio in c:\\users\\riina\\anaconda3\\lib\\site-packages (0.2.14)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\riina\\anaconda3\\lib\\site-packages (from speechrecognition) (4.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\riina\\anaconda3\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\riina\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\riina\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\riina\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\riina\\anaconda3\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\riina\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\riina\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install speechrecognition datasets faiss-cpu PyAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# travel_assistant.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from datasets import load_dataset\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize environment\n",
    "load_dotenv()\n",
    "recognizer = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Processing and Vector Store Setup\n",
    "def initialize_knowledge_base():\n",
    "    embed = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    \n",
    "    if not os.path.exists(\"faiss.index\"):\n",
    "        # Load and process data\n",
    "        data = load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train[:100]\", trust_remote_code=True)\n",
    "        \n",
    "        def create_document(article):\n",
    "            return Document(\n",
    "                page_content=article[\"text\"],\n",
    "                metadata={\"title\": article[\"title\"]}\n",
    "            )\n",
    "        \n",
    "        documents = [create_document(raw) for raw in data]\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "        split_docs = splitter.split_documents(documents)\n",
    "        \n",
    "        # Create and save vector store\n",
    "        vectorstore = FAISS.from_documents(split_docs, embed)\n",
    "        vectorstore.save_local(\"faiss.index\")\n",
    "    \n",
    "    # Load existing vector store\n",
    "    return FAISS.load_local(\"faiss.index\", embed, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voice_to_text():\n",
    "    try:\n",
    "        with sr.Microphone() as source:\n",
    "            print(\"\\nListening... (Speak now)\")\n",
    "            audio = recognizer.listen(source, timeout=10)\n",
    "        return recognizer.recognize_google(audio)\n",
    "    except sr.WaitTimeoutError:\n",
    "        print(\"Listening timed out\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Audio error: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_user_input():\n",
    "    input_type = input(\"\\nChoose input method [text/audio]: \").lower()\n",
    "    return voice_to_text() if input_type == \"audio\" else input(\"Your question: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Response Formatting\n",
    "def format_response(response):\n",
    "    formatted = f\"\\nANSWER: {response['result']}\"\n",
    "    sources = {doc.metadata['title'] for doc in response['source_documents']}\n",
    "    if sources:\n",
    "        formatted += \"\\n\\nSources:\\n- \" + \"\\n- \".join(sources)\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸŒŸ Travel Assistant Activated ðŸŒŸ\n",
      "Ask about any city or request travel plans!\n",
      "Examples:\n",
      "- What's special about Tokyo?\n",
      "- 3-day itinerary for Rome\n",
      "- Tell me about New York's museums\n",
      "==================================================\n",
      "\n",
      "Listening... (Speak now)\n",
      "\n",
      "ANSWER:  I don't know.\n",
      "\n",
      "Sources:\n",
      "- Afghanistan\n",
      "- Algebra\n",
      "- Armenia\n",
      "- Belgium\n",
      "\n",
      "Listening... (Speak now)\n",
      "\n",
      "ANSWER:  London is a metropolis that includes several small ancient towns and villages, including London, Westminster, and many old villages such as Notting Hill, Southwark, Richmond, Greenwich, etc. The part that is officially known as the \"City of London\" only takes up one square mile, while the rest is known as \"Greater London.\" It is one of the world's largest cities and has been continually changing for hundreds of years.\n",
      "\n",
      "Sources:\n",
      "- April\n",
      "- City\n",
      "\n",
      "Listening... (Speak now)\n",
      "Audio error: \n",
      "\n",
      "ðŸš— Safe travels! Closing assistant...\n"
     ]
    }
   ],
   "source": [
    "# 4. Main Application\n",
    "def main():\n",
    "    # Initialize components\n",
    "    vectorstore = initialize_knowledge_base()\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=OpenAI(temperature=0.2, model_name=\"gpt-3.5-turbo-instruct\"),\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸŒŸ Travel Assistant Activated ðŸŒŸ\")\n",
    "    print(\"Ask about any city or request travel plans!\")\n",
    "    print(\"Examples:\")\n",
    "    print(\"- What's special about Tokyo?\")\n",
    "    print(\"- 3-day itinerary for Rome\")\n",
    "    print(\"- Tell me about New York's museums\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            query = get_user_input()\n",
    "            if not query:\n",
    "                continue\n",
    "\n",
    "            # Process query\n",
    "            if any(kw in query.lower() for kw in [\"plan\", \"itinerary\", \"schedule\"]):\n",
    "                response = qa_chain.invoke({\"query\": f\"Create detailed travel plan: {query}\"})\n",
    "            else:\n",
    "                response = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "            # Show response\n",
    "            print(format_response(response))\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nðŸš— Safe travels! Closing assistant...\")\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyAudio\n",
      "  Downloading PyAudio-0.2.14-cp312-cp312-win_amd64.whl.metadata (2.7 kB)\n",
      "Downloading PyAudio-0.2.14-cp312-cp312-win_amd64.whl (164 kB)\n",
      "Installing collected packages: PyAudio\n",
      "Successfully installed PyAudio-0.2.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
